{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Zz3mlyOMlACi",
        "outputId": "afbe7668-043d-432c-fa1b-40cf123627a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.chdir('/content/')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Reading data\n",
        "buy_data_path = '/content/data/buy_orders/buy_history_2025-12-15.csv'\n",
        "sell_data_path = '/content/data/sell_orders/sell_history_2025-12-15.csv'"
      ],
      "metadata": {
        "id": "B_qLa0afmxOB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buys"
      ],
      "metadata": {
        "id": "pJ81DgsfIJIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buys = pd.read_csv(buy_data_path)\n",
        "buys.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "sSOAHXTVmFbd",
        "outputId": "b2e21dfc-3a79-4826-8f38-70658bcf79f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/buy_orders/buy_history_2025-12-15.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4036096448.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuy_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbuys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/buy_orders/buy_history_2025-12-15.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buys.groupby('item_name').agg({'quantity':'sum'}).sort_values('quantity', ascending = False)"
      ],
      "metadata": {
        "id": "Fhecb2nWoIPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sells"
      ],
      "metadata": {
        "id": "2D8Fp-MRINt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sells = pd.read_csv(sell_data_path)\n",
        "sells.head()"
      ],
      "metadata": {
        "id": "KCi9XR56m1-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sells.groupby('item_name').agg({'quantity':'sum'}).sort_values('quantity', ascending = False)"
      ],
      "metadata": {
        "id": "U_Q1NPA3IQHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "# Combines all buy history files\n",
        "all_files = glob.glob('data/buy_orders/buy_history_*.csv')\n",
        "dfBuy = pd.concat([pd.read_csv(f) for f in all_files])\n",
        "dfBuy = dfBuy.drop_duplicates()  # remove any duplicates\n",
        "\n",
        "# Combines all sell history files\n",
        "all_files = glob.glob('data/sell_orders/sell_history_*.csv')\n",
        "dfSell = pd.concat([pd.read_csv(f) for f in all_files])\n",
        "dfSell = dfSell.drop_duplicates()  # remove any duplicates"
      ],
      "metadata": {
        "id": "Ltar-3ottSav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bought_demo = ['100 of sigil of bloodlust 8. 12.2025', '24 of sigil of bloodlust 8. 12.2025']\n",
        "sold_demo = ['50 of sigil of bloodlust 8. 12.2025', '23 of sigil of bloodlust 9. 12.2025', '11 of sigil of bloodlust 10. 12.2025', '2 of sigil of bloodlust 12. 12.2025', '33 of sigil of bloodlust 16. 12.2025' ]#put the last one on purpose more than 7 days in the future\n",
        "#not sure if i should include the price it was filled, guess it doesnt hurt but I usually sell at market price -1 copper to squeeze out margin. Like maybe the model could predict a sale faster at a different price point, but eh, that seems more nice to have. What I want is to know how fast its gonna sell at market price-1c, so i guess for this use case it might not be important"
      ],
      "metadata": {
        "id": "1dFk3Pfhx1ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Survival analysis with Weibull Distribution"
      ],
      "metadata": {
        "id": "mtKJk3piJ2Lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$f(t) = \\frac{k}{\\lambda} \\left( \\frac{t}{\\lambda} \\right)^{k-1} e^{-\\left( \\frac{t}{\\lambda} \\right)^k}$$\n",
        "\n",
        "This function shows how failure probability density changes over time, with the shape parameter k determining if failures are more likely early (k < 1), constant over time (k = 1), or increasing with age (k > 1).\n",
        "\n",
        "The cumulative distribution function (CDF) gives the probability that failure occurs by a specific time:\n",
        "$$F(t) = 1 - e^{-\\left( \\frac{t}{\\lambda} \\right)^k}$$\n",
        "\n",
        "This function works well for calculating the percentage of items expected to fail within a given timeframe. This is what I need for my use case. I need to see if, say, 90% get sold ('fail') within a week. However, is this the same thing as probability to fill? I feel an itch to use the Bayes theorem but I am not completely sure.\n",
        "\n",
        " The reliability function (1-CDF) provides the complementary perspective, showing the probability of survival beyond time t.\n",
        "\n",
        "The hazard function reveals the instantaneous failure rate at any given time. This is useful to model undercut probability after a certain time period should I ever need this.\n",
        "\n",
        "$$h(t) = \\frac{k}{\\lambda} \\left( \\frac{t}{\\lambda} \\right)^{k-1}$$"
      ],
      "metadata": {
        "id": "GqwgnFyqM_Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lifelines.fitters.weibull_fitter import WeibullFitter\n",
        "#!pip install lifelines\n",
        "from lifelines import WeibullFitter as wf\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def fit_item_models(df,min_observations=3):\n",
        "  #@ min observations is the minimum number of items sold to fit a model\n",
        "  \"\"\"  FIts WEibull distribution on each item in the list given enough data\"\"\"\n",
        "  item_distributions = {}\n",
        "\n",
        "  # Groupby item\n",
        "  for item_id, group in df.groupby('item_name'):\n",
        "      if len(group) < min_observations: continue\n",
        "      item_name = group['item_name'].iloc[0]\n",
        "      durations = group['time_to_fill_hours'].dropna()\n",
        "\n",
        "      #WEibull fit\n",
        "      wf = WeibullFitter()\n",
        "      wf.fit(durations)\n",
        "\n",
        "      item_distributions[int(item_id)] = {\n",
        "          'item_name':item_name,\n",
        "          'lambda_': float(wf.lambda_),#scale parameter\n",
        "          'rho_': float(wf.rho_),#shape parameter\n",
        "          'n_observations': len(durations),\n",
        "          'median_fill_hours': float(durations.median()),\n",
        "          'mean_fill_hours': float(durations.mean()),\n",
        "          'std_fill_hours': float(durations.std()),\n",
        "      }\n",
        "  return item_distributions\n",
        ""
      ],
      "metadata": {
        "id": "HZ7PR15CEwMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit models\n",
        "print(\"Fitting models  per item...\")\n",
        "models = fit_item_models(dfSell,min_observations=3)\n",
        "print(\"Done for {len(models)} items\")\n",
        "#Save to JSON\n",
        "with open('data/item_distributions.json', 'w') as f:\n",
        "    json.dump(models, f, indent=2)\n",
        "print(\"Models saved\")"
      ],
      "metadata": {
        "id": "Yr-mFw7fVIOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fill_probability(lambda_, rho_, hours):\n",
        "    \"\"\"\n",
        "    Calculate P(item fills within 'hours' hours) using Weibull CDF\n",
        "    F(t) = 1 - exp(-(t/lambda)^rho)\n",
        "    \"\"\"\n",
        "    return 1 - np.exp(-((hours / lambda_) ** rho_))\n",
        "\n",
        "# Example: What's the probability each item fills within 24 hours?\n",
        "time_horizon_hours = 24\n",
        "\n",
        "print(f\"\\nFill probabilities within {time_horizon_hours} hours:\")\n",
        "for item_id, model in list(models.items())[:10]:  # Show first 10\n",
        "    prob = calculate_fill_probability(model['lambda_'], model['rho_'], time_horizon_hours)\n",
        "    print(f\"{model['item_name']}: {prob:.1%} (shape={model['rho_']:.2f})\")"
      ],
      "metadata": {
        "id": "RYdWsJWzVqt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXVprRDFs86v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizer\n",
        "I guess this should be a separate file later? If I do end up saving the probabilities somewhere to save time and recalculate only occasionally."
      ],
      "metadata": {
        "id": "hR8b_LMrMlzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For optimizer\n",
        "budget = 1000 #gold\n",
        "time_horizon = 1 #days\n",
        "min_margin=.05\n",
        "min_fill_prob = .9\n",
        "max_transactions = 50 #batch of up to 250 items. This is to prevent optimizer solutions that would have me click through 1000 buy orders.\n",
        "\n",
        "z = ...#sum of sell price of items meeting the margin threshold and min_fill_prob\n",
        "full_item_list = pd.read_csv('data/my_trading_items.csv')[[\"item_name\"]] #not even sure what I did this for yet\n",
        "print(full_item_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "jsmgbVx4FJ4I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}